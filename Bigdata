Big data-- 
    - Huge Amount
    - type of data
        1. Structered - DBMS, Xls, CSV
        2. Semi Structered - HTML JSON XML
        3. Un Structered - Image, Video --> No Use for Computer or general computer cant understand without ML.

    Bigdata - Problem
    Hadoop - Solution

    Data --> [Process] --> Info.

    Hadoop --> Framework 

    Problem 1-
        Storage -- > HDFS --> Hadoop distributed file system

                                                         |--- Slave(data Node)  
                                                         |--- Slave(data Node)
        data generaotor ---Master(Name Node /metadata) --|--Slave(data Node)
                                                         |-- Slave(data Node)
                                                         |--Slave(data Node)
                fig- Hadoop Cluster
        
        Master- Name Node -- High CPU, RAM low Harddisk
        Slave - Data Node -- LOW CPU, RAM (even 5 mb) , High HARD Disk


    Google - Cloop -- Cloud + Hadoop

    Common steps - For name and data

    hadoop2/etc/hadoop/hadoop-env.sh --> To tell hadoop the path of the java
     " /hdfs-site.xml --> To tell about name node and data node.

     Name Node -format. A structure to store info or metadata . A fix format.
     hdfs-
     Name Node ip : 50070 is the hdfs portal by default
     Hadoop2 --> default Minimum Block size - 128
                Default replecation factor - 3

        WORM -- Write once read many -- File cannot be overwritten . Delete Previous and Upload new. 

         hdfs dfs -ls /
         hdfs dfs -mkdir /facebook 

         hdfs dfs -put aa.txt /facebook


         blog --> slashbigdata ---> Hadoop1 vs hadoop2

         Single node cluster -- Datanode and namenode on single machine
         Multinode cluster -- different different datanode and name node

         SudoDistributed cluster 
         Fully Distributed cluster
         No distributed cluster

    Setup hadoop.. 
            Download jdk and install via rpm 
            Download hadoop tar.gz file unzip to /hadoop2
        set java and hadoop path to 
            /root/.bashrc
                JAVA_HOME=/usr/java/jdk1.8...
            and /hadoop2/etc/hadoop/hadoop-env.sh

YARN ---> Distributed data processing --
    [Resource manager ---(scheduler, Application manager)]

    [node manager -- (container/jvm) + (app Master)]

hdfs dfs -moveFromLocal #data to be moved to hdfs# #hadoop directory where data is to be moved#

cd /hadoop2/share/hadoop/mapreduece 
yarn jar hadoop-mapreduce-examples ---> To run jar file in hadoop user yarn jar ---> to check all function s available

To run a function --

yarn jar hadoop-mapreduce-examples ... #function like word count# # data to be processed path # #a new directory to store output # #Word to be searched if grep or any thing similar used#

Scheduler ---
    1. FIFO -Hadoop1
    2. FAIR --Hadoop2 -Yarn default 
    3. Capacity scheduler when capacity or resource needed for jop is known already 

Map -reduces -- Two functions to get hte job done 
    Works in key pair

    Mapper --
        Splits data row wise 
        Mapping 
        shuffling
    Now Reducer
    Final result 


DATA ANALYTICS --->
    Analytics and visualization ---> Apaache databricks.
                                            /\
                            Apache spark --<  >--- AWS

    Big data hadoop -->     HDFS + YARN + HIVE (SQL Process) +

    Matplotlib + Apache Mahout(For ML on data bricks)  + Big data ==> Data bricks 

    Free --> 15 days free trial for an email + 6 GB ram instance on AWS free .

Python data visualization or analysis 
    Matplotlib 
    Seaborn 
    Pyplot 

Apache spark is backend of Databricks.

Reason behind speed of spark --> In memory processing --> processes data in RAM, 

Apache Spark --> 
    supports upto 4 languages -
        1. Python - can process max upto -- 10GB -> Pyspark 
        2. Scala --> can process more tahn java.  
        3. R
        4. java
    spark designed in scala language.

jupyter notebook 
import findspark
findspark.init('/spark')
import pyspark
from pyspark import Spark.Context //Blog --> slashbigdata

x = SparkContext
frdd = x.textFile('/etc/passwd')

frdd.first()
//For first line 
frdd.take(3)
// For first 3 lines.f




    // pyhton --> findspark --> spark --> pyspark 

RDD --> Resilient Distributed datasets --> Main component of spark 
    File loaded in ram by one user and one program , it will be distributed over ram on entire cluster. 
    lazy -- RDD --> Main concept of spark.
    We hav to make RDD in spark -- it is state of data which could be after either loading file after word count and etc..

    spark ----> Core operation
                    \--> action --> Performs action on data
                     \--> transform  --> Convert data to spark understandable manner
    Spark --> HDFS + YARN
                |
                \
                 > WORM --> Write once read many
    Spark has its own storage it can use it and it also can use hadoop as storage.

    -> Spark SQL --> 
            \-->
    
    -> Spark MLib -->
            \-->    Machine learning

    -> Spark Dgraph
            \--> Matplotlib is better
    -> Spark Streaming 
            \--> For live data streaming 
    
    Spark uses in memory function thats why it is 100X Faster than other big data analytics techniques like Hadoop and Yarn.


Study Google  --> Lambda function in python --> lambda function
    --> file- wordcount - graph


